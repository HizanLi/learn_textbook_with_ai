import json
import os
import logging
from pathlib import Path
from typing import List, Dict
import chromadb
from chromadb.utils import embedding_functions
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VectorStorageManager:
    def __init__(self, db_path: str = "./chroma_db", collection_name: str = "liquidity_theory"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†
        :param db_path: æœ¬åœ°æ•°æ®åº“å­˜å‚¨è·¯å¾„
        :param collection_name: å‘é‡é›†åˆåç§°
        """
        # 1. åˆå§‹åŒ– ChromaDB æŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=db_path)
        
        # 2. å®šä¹‰ Embedding å‡½æ•° (ä½¿ç”¨æœ¬åœ° Sentence-Transformers æ¨¡å‹)
        # all-MiniLM-L6-v2 æ˜¯ä¸€ä¸ªè½»é‡ä¸”é«˜æ•ˆçš„é€šç”¨æ¨¡å‹ï¼Œé€‚åˆå¤„ç†ä¸­è‹±åŒè¯­æˆ–ä¸“ä¸šä¹¦ç±
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # 3. åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=self.embedding_fn
        )

    def load_chunks(self, json_path: str) -> List[Dict]:
        """è¯»å– chunks.json æ–‡ä»¶"""
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆ†å—æ–‡ä»¶: {json_path}")
        with open(json_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def process_and_store(self, chunks: List[Dict]):
        """
        æ‰§è¡Œæ ‡é¢˜æ³¨å…¥å¹¶å…¥åº“ï¼Œä¿®å¤å…ƒæ•°æ®ä¸­ç©ºåˆ—è¡¨å¯¼è‡´çš„é”™è¯¯
        """
        documents = []
        metadatas = []
        ids = []

        logger.info(f"å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†å—å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡...")

        for i, chunk in enumerate(tqdm(chunks)):
            content = chunk["content"]
            meta = chunk["metadata"].copy() # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
            
            # --- ä¿®å¤é€»è¾‘ï¼šå¤„ç†ç©ºåˆ—è¡¨ ---
            # ChromaDB å…ƒæ•°æ®ä¸æ”¯æŒç©ºåˆ—è¡¨ã€‚æˆ‘ä»¬å°†åˆ—è¡¨è½¬ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ã€‚
            if "referenced_images" in meta:
                if isinstance(meta["referenced_images"], list):
                    # å¦‚æœåˆ—è¡¨ä¸ä¸ºç©ºï¼Œç”¨é€—å·æ‹¼æ¥ï¼›å¦‚æœä¸ºç©ºï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²
                    meta["referenced_images"] = ", ".join(meta["referenced_images"])
            
            # --- æ ‡é¢˜è·¯å¾„æ³¨å…¥ ---
            headers = [meta.get("header_1", ""), meta.get("header_2", ""), meta.get("header_3", "")]
            header_path = " > ".join([h for h in headers if h]).strip()
            
            enriched_text = f"Section: {header_path}\nContent: {content}"
            
            documents.append(enriched_text)
            metadatas.append(meta)
            ids.append(f"chunk_{i}")

        # åˆ†æ‰¹å†™å…¥æ•°æ®åº“
        batch_size = 100
        for j in range(0, len(documents), batch_size):
            self.collection.add(
                documents=documents[j : j + batch_size],
                metadatas=metadatas[j : j + batch_size],
                ids=ids[j : j + batch_size]
            )

        logger.info(f"âœ… æˆåŠŸå‘é‡åŒ– {len(documents)} ä¸ªåˆ†å—å¹¶ä¿å­˜ã€‚")

    def search(self, query_text: str, n_results: int = 3):
        """æ‰§è¡Œè¯­ä¹‰æœç´¢æµ‹è¯•"""
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results
        )
        return results

# --- è¿è¡Œä¸»æµç¨‹ ---
if __name__ == "__main__":
    # 1. å®ä¾‹åŒ–ç®¡ç†å™¨
    manager = VectorStorageManager()
    
    try:
        # 2. åŠ è½½ä¹‹å‰ç”Ÿæˆçš„ chunks.json
        # ç¡®ä¿è¯¥æ–‡ä»¶åœ¨è„šæœ¬åŒçº§ç›®å½•ä¸‹ï¼Œæˆ–æä¾›å®Œæ•´è·¯å¾„
        json_file_path = r"D:\mineru_test\output\pyhton_short\hybrid_auto\chunks.json" 
        data = manager.load_chunks(json_file_path)
        
        # 3. æ‰§è¡Œå‘é‡åŒ–å’Œå­˜å‚¨
        manager.process_and_store(data)
        
        # 4. éªŒè¯æµ‹è¯•
        print("\n" + "="*50)
        print("ğŸ” æ£€ç´¢åŠŸèƒ½æ¼”ç¤ºï¼š")
        
        # æµ‹è¯•ï¼šé’ˆå¯¹ä¹¦ä¸­å…·ä½“æ¦‚å¿µæé—®
        test_queries = [
            "What is Conditional execution",
            "What are reserved words in Python?",
            "What are the rules and restrictions for naming variables in Python?",
            "How do you define a Boolean expression?",
        ]
        
        for q in test_queries:
            print(f"\nç”¨æˆ·æé—®: {q}")
            results = manager.search(q, n_results=1)
            
            if results['documents']:
                matched_doc = results['documents'][0][0]
                matched_meta = results['metadatas'][0][0]
                print(f"åŒ¹é…ç« èŠ‚: {matched_meta.get('header_1')} -> {matched_meta.get('header_2')}")
                print(f"æ‰¾åˆ°å†…å®¹: {matched_doc}")
                print()
                print()
        
        print("\n" + "="*50)
        
    except Exception as e:
        logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")