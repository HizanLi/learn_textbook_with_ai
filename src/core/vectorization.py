import json
import os
import logging
from pathlib import Path
from typing import List, Dict
import chromadb
from chromadb.utils import embedding_functions
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VectorStorageManager:
    def __init__(self, collection_name: str, db_path: str = "./chroma_db"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“ç®¡ç†
        :param collection_name: å‘é‡é›†åˆåç§°
        :param db_path: æœ¬åœ°æ•°æ®åº“å­˜å‚¨è·¯å¾„å‰ç¼€
        """
        self.collection_name = collection_name
        # ä¸ºæ¯ä¸ª collection åˆ›å»ºå•ç‹¬çš„ db æ–‡ä»¶å¤¹
        self.db_path = f"{db_path}/{collection_name}"
        
        # 1. åˆå§‹åŒ– ChromaDB æŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=self.db_path)
        
        # 2. å®šä¹‰ Embedding å‡½æ•° (ä½¿ç”¨æœ¬åœ° Sentence-Transformers æ¨¡å‹)
        # all-MiniLM-L6-v2 æ˜¯ä¸€ä¸ªè½»é‡ä¸”é«˜æ•ˆçš„é€šç”¨æ¨¡å‹ï¼Œé€‚åˆå¤„ç†ä¸­è‹±åŒè¯­æˆ–ä¸“ä¸šä¹¦ç±
        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # 3. åˆ›å»ºæˆ–è·å–é›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=self.embedding_fn
        )

    def load_chunks(self, json_path: str) -> List[Dict]:
        """è¯»å– chunks.json æ–‡ä»¶"""
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆ†å—æ–‡ä»¶: {json_path}")
        with open(json_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def process_and_store(self, chunks: List[Dict]):
        """
        æ‰§è¡Œæ ‡é¢˜æ³¨å…¥å¹¶å…¥åº“ï¼Œä¿®å¤å…ƒæ•°æ®ä¸­ç©ºåˆ—è¡¨å¯¼è‡´çš„é”™è¯¯
        å¦‚æœé›†åˆå·²æœ‰æ•°æ®ï¼Œåˆ™è·³è¿‡å‘é‡åŒ–ï¼ˆå¹‚ç­‰æ€§ï¼‰
        """
        # æ£€æŸ¥é›†åˆæ˜¯å¦å·²æœ‰æ•°æ®
        collection_count = self.collection.count()
        if collection_count > 0:
            logger.info(f"â­ï¸  é›†åˆ '{self.collection_name}' å·²å­˜åœ¨ {collection_count} ä¸ªåˆ†å—ï¼Œè·³è¿‡å‘é‡åŒ–ã€‚")
            return
        
        documents = []
        metadatas = []
        ids = []

        logger.info(f"å¼€å§‹å¤„ç† {len(chunks)} ä¸ªåˆ†å—å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡...")

        for i, chunk in enumerate(tqdm(chunks)):
            content = chunk["content"]
            meta = chunk["metadata"].copy() # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
            
            # --- ä¿®å¤é€»è¾‘ï¼šå¤„ç†ç©ºåˆ—è¡¨ ---
            # ChromaDB å…ƒæ•°æ®ä¸æ”¯æŒç©ºåˆ—è¡¨ã€‚æˆ‘ä»¬å°†åˆ—è¡¨è½¬ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ã€‚
            if "referenced_images" in meta:
                if isinstance(meta["referenced_images"], list):
                    # å¦‚æœåˆ—è¡¨ä¸ä¸ºç©ºï¼Œç”¨é€—å·æ‹¼æ¥ï¼›å¦‚æœä¸ºç©ºï¼Œè®¾ä¸ºç©ºå­—ç¬¦ä¸²
                    meta["referenced_images"] = ", ".join(meta["referenced_images"])
            
            # --- æ ‡é¢˜è·¯å¾„æ³¨å…¥ ---
            headers = [meta.get("header_1", ""), meta.get("header_2", ""), meta.get("header_3", "")]
            header_path = " > ".join([h for h in headers if h]).strip()
            
            enriched_text = f"Section: {header_path}\nContent: {content}"
            
            documents.append(enriched_text)
            metadatas.append(meta)
            ids.append(f"chunk_{i}")

        # åˆ†æ‰¹å†™å…¥æ•°æ®åº“
        batch_size = 100
        for j in range(0, len(documents), batch_size):
            self.collection.add(
                documents=documents[j : j + batch_size],
                metadatas=metadatas[j : j + batch_size],
                ids=ids[j : j + batch_size]
            )

        logger.info(f"âœ… æˆåŠŸå‘é‡åŒ– {len(documents)} ä¸ªåˆ†å—å¹¶ä¿å­˜ã€‚")

    def search(self, query_text: str, n_results: int = 3):
        """æ‰§è¡Œè¯­ä¹‰æœç´¢å¹¶æŒ‰å†…å®¹è´¨é‡æ’åº"""
        # è·å–æ›´å¤šç»“æœç”¨äºé‡æ’åº
        fetch_count = min(max(n_results * 3, 10), 50)  # è·å–n_resultsçš„3å€æˆ–æœ€å¤š50ä¸ª
        results = self.collection.query(
            query_texts=[query_text],
            n_results=fetch_count
        )
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œç›´æ¥è¿”å›
        if not results.get('documents') or len(results['documents']) == 0:
            return results
        
        # é‡æ’åºï¼šæŒ‰distanceå‡åºï¼Œä½†ä¼˜å…ˆè€ƒè™‘å†…å®¹é•¿åº¦
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        distances = results.get('distances', [[]])[0]
        
        # åˆ›å»ºæ’åºå…ƒç»„åˆ—è¡¨
        items = list(zip(documents, metadatas, distances))
        
        # æ’åºç­–ç•¥ï¼šå…ˆæŒ‰å†…å®¹é•¿åº¦ï¼ˆé™åºï¼‰ï¼Œå†æŒ‰distanceï¼ˆå‡åºï¼‰
        # è¿™æ ·ä¼šä¼˜å…ˆè¿”å›å†…å®¹æ›´ä¸°å¯Œçš„ç»“æœ
        items.sort(key=lambda x: (
            -len(x[0]),  # å†…å®¹é•¿åº¦é™åºï¼ˆè´Ÿå·ä½¿å…¶é™åºï¼‰
            x[2]          # distanceå‡åº
        ))
        
        # åªä¿ç•™å‰n_resultsä¸ªç»“æœ
        items = items[:n_results]
        
        # é‡æ–°æ‹†åˆ†å›åŸæ ¼å¼
        sorted_docs, sorted_metas, sorted_dists = zip(*items) if items else ([], [], [])
        
        return {
            'documents': [list(sorted_docs)],
            'metadatas': [list(sorted_metas)],
            'distances': [list(sorted_dists)]
        }
    
    def collection_exists(self) -> bool:
        """æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®"""
        try:
            count = self.collection.count()
            return count > 0
        except Exception:
            return False

# # --- è¿è¡Œä¸»æµç¨‹ ---
# if __name__ == "__main__":
#     # 1. å®ä¾‹åŒ–ç®¡ç†å™¨
#     manager = VectorStorageManager()
    
#     try:
#         # 2. åŠ è½½ä¹‹å‰ç”Ÿæˆçš„ chunks.json
#         # ç¡®ä¿è¯¥æ–‡ä»¶åœ¨è„šæœ¬åŒçº§ç›®å½•ä¸‹ï¼Œæˆ–æä¾›å®Œæ•´è·¯å¾„
#         json_file_path = r"D:\mineru_test\output\pyhton_short\hybrid_auto\chunks.json" 
#         data = manager.load_chunks(json_file_path)
        
#         # 3. æ‰§è¡Œå‘é‡åŒ–å’Œå­˜å‚¨
#         manager.process_and_store(data)
        
#         # 4. éªŒè¯æµ‹è¯•
#         print("\n" + "="*50)
#         print("ğŸ” æ£€ç´¢åŠŸèƒ½æ¼”ç¤ºï¼š")
        
#         # æµ‹è¯•ï¼šé’ˆå¯¹ä¹¦ä¸­å…·ä½“æ¦‚å¿µæé—®
#         test_queries = [
#             "What is Conditional execution",
#             "What are reserved words in Python?",
#             "What are the rules and restrictions for naming variables in Python?",
#             "How do you define a Boolean expression?",
#         ]
        
#         for q in test_queries:
#             print(f"\nç”¨æˆ·æé—®: {q}")
#             results = manager.search(q, n_results=1)
            
#             if results['documents']:
#                 matched_doc = results['documents'][0][0]
#                 matched_meta = results['metadatas'][0][0]
#                 print(f"åŒ¹é…ç« èŠ‚: {matched_meta.get('header_1')} -> {matched_meta.get('header_2')}")
#                 print(f"æ‰¾åˆ°å†…å®¹: {matched_doc}")
#                 print()
#                 print()
        
#         print("\n" + "="*50)
        
#     except Exception as e:
#         logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")