import os
import re
import json
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Any
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

class MarkdownChunker:
    def __init__(self):
        self.base_output_dir = Path(os.getenv("OUTPUT_DIR", "D:/mineru_test/output"))
        self.sub_dir_patterns = ["hybrid_auto", "hybrid_ocr", "hybrid_txt"]
        
        # æ ‡é¢˜åˆ‡åˆ†å™¨ï¼šä¿ç•™ä¹¦ä¸­çš„é€»è¾‘ç»“æ„
        headers_to_split_on = [
            ("#", "Header_1"),
            ("##", "Header_2"),
            ("###", "Header_3"),
        ]
        self.md_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on, 
            strip_headers=False 
        )
        
        # é€’å½’åˆ‡åˆ†å™¨ï¼šç¡®ä¿å—å¤§å°é€‚åˆå‘é‡åŒ–
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
        )

    def _find_md_file(self, file_name: str) -> Path:
        file_stem = Path(file_name).stem
        for pattern in self.sub_dir_patterns:
            search_pattern = f"**/{pattern}/{file_stem}.md"
            matches = list(self.base_output_dir.glob(search_pattern))
            if matches:
                return matches[0]
        raise FileNotFoundError(f"æœªèƒ½åœ¨ {self.base_output_dir} ä¸‹æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {file_name}")

    def get_chunks(self, file_name: str) -> List[Dict[str, Any]]:
        target_path = self._find_md_file(file_name)
        with open(target_path, "r", encoding="utf-8") as f:
            content = f.read()

        header_splits = self.md_splitter.split_text(content)
        chunk_list = []

        for doc in header_splits:
            sub_docs = self.text_splitter.split_documents([doc])
            for sub_doc in sub_docs:
                # æå–å›¾ç‰‡å¼•ç”¨
                img_pattern = r"!\[.*?\]\((images/.*?)\)"
                images = re.findall(img_pattern, sub_doc.page_content)
                
                # æ„é€ åºåˆ—åŒ–å­—å…¸
                chunk_data = {
                    "content": sub_doc.page_content,
                    "metadata": {
                        "source": file_name,
                        "header_1": sub_doc.metadata.get("Header_1", ""),
                        "header_2": sub_doc.metadata.get("Header_2", ""),
                        "header_3": sub_doc.metadata.get("Header_3", ""),
                        "referenced_images": images,
                        "has_image": len(images) > 0
                    }
                }
                chunk_list.append(chunk_data)
        
        return chunk_list

def save_chunks_to_json(chunks: List[Dict], output_path: str):
    """å°†åˆ†å—åˆ—è¡¨ä¿å­˜ä¸º JSON æ–‡ä»¶"""
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(chunks)} ä¸ªåˆ†å—åˆ°: {output_path}")

# --- æ‰§è¡Œå­˜å‚¨ ---
if __name__ == "__main__":
    chunker = MarkdownChunker()
    try:
        # 1. è·å–åˆ‡åˆ†åçš„æ•°æ®
        target_file = "book.md" 
        all_chunks = chunker.get_chunks(target_file)
        
        # 2. å®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆå»ºè®®ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•æˆ–è¾“å‡ºç›®å½•æ ¹éƒ¨ï¼‰
        json_output = "chunks.json" 
        
        # 3. æ‰§è¡Œå­˜å‚¨
        save_chunks_to_json(all_chunks, json_output)

    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå¤±è´¥: {e}")