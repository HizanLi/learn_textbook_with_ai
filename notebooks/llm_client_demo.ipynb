{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a982280b",
   "metadata": {},
   "source": [
    "# LLM Client Usage Demo\n",
    "\n",
    "This notebook demonstrates how to use the `llm_client.py` module to create and interact with various LLM providers (OpenAI, Deepseek, Gemini) via the `LLMFactory`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18cb83",
   "metadata": {},
   "source": [
    "## 2. Import LLM Client Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231f2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ensure repo root is on path\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"\")))\n",
    "\n",
    "from src.core.llm_client import ModelProvider, LLMFactory, OpenAIClient, DeepseekClient, GeminiClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803786d",
   "metadata": {},
   "source": [
    "## 3. Configure Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408eefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys loaded? True True True\n"
     ]
    }
   ],
   "source": [
    "# either set in this cell (not recommended for secrets) or create a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# example assignments (uncomment and fill with real keys if desired):\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-key'\n",
    "# os.environ['DEEPSEEK_API_KEY'] = 'your-deepseek-key'\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-gemini-key'\n",
    "\n",
    "print(\"keys loaded?\", \n",
    "      'OPENAI_API_KEY' in os.environ, \n",
    "      'DEEPSEEK_API_KEY' in os.environ, \n",
    "      'GEMINI_API_KEY' in os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80751279",
   "metadata": {},
   "source": [
    "## 4. Instantiate Clients with Different Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95962e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.core.llm_client.OpenAIClient object at 0x1218e2710> <src.core.llm_client.DeepseekClient object at 0x11237f9d0> <src.core.llm_client.GeminiClient object at 0x112b50f50>\n"
     ]
    }
   ],
   "source": [
    "# create clients via the factory\n",
    "openai_client = LLMFactory.create_client(ModelProvider.OPENAI, temperature=0.3)\n",
    "deepseek_client = LLMFactory.create_client(ModelProvider.DEEPSEEK, model_name='deepseek-chat', temperature=0.5)\n",
    "gemini_client = LLMFactory.create_client(ModelProvider.GOOGLE, model_name='gemini-flash-latest', temperature=0.7)\n",
    "\n",
    "print(openai_client, deepseek_client, gemini_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a7e39",
   "metadata": {},
   "source": [
    "## 4.1 Notes on warnings\n",
    "\n",
    "## 5. Generate Text with Each Client\n",
    "\n",
    "You may see output like object memory addresses or deprecation warnings. The memory addresses (e.g. `<src.core.llm_client.OpenAIClient object at ...>`) are just Python's default `repr` for the instances; the above change prints a friendly summary instead.\n",
    "\n",
    "Tqdm warnings can be resolved by installing and enabling `ipywidgets` in your Jupyter environment. The Google package deprecation message indicates you should migrate from `google.generativeai` to `google.genai` when updating your dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OpenAI ---\n",
      "\n",
      "--- Deepseek ---\n",
      "Embrace the boundless curiosity of artificial intelligence as your companion, for it transforms the vast, uncharted ocean of human knowledge into a navigable river, lighting the path with instant insights and revealing patterns once hidden in the dark. Let this powerful synergy ignite a lifelong fire within you, where every question becomes a doorway and every interaction with a machine deepens your own uniquely human capacity for wonder, creativity, and profound understanding.\n",
      "When we integrate artificial intelligence into our educational journey, we transform the vast ocean of global information into a personalized stream of profound insight and endless discovery. This symbiotic relationship between human curiosity and machine intelligence serves as a luminous beacon, guiding us toward a new era where the potential for growth is limited only by the reach of our imagination.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Please give me a twoâ€‘sentence inspirational quote about learning with AI.  Make it at least 50 words.\"\n",
    "system = \"You are a helpful assistant.\"\n",
    "\n",
    "# print(\"--- OpenAI ---\")\n",
    "# print(openai_client.generate_text(prompt, system_prompt=system))\n",
    "# print(openai_client.generate_text(prompt, system_prompt=system, temperature=0.5))\n",
    "\n",
    "print(\"\\n--- Deepseek ---\")\n",
    "print(deepseek_client.generate_text(prompt, system_prompt=system, temperature=0.5))\n",
    "\n",
    "print(\"\\n--- Gemini ---\")\n",
    "print(gemini_client.generate_text(prompt, system_prompt=system, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d87fd8",
   "metadata": {},
   "source": [
    "## 6. Generate JSON with Each Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656c28dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OpenAI JSON response ---\n",
      "{'message': 'Believe in yourself and all that you are. Know that there is something inside you that is greater than any obstacle.', 'length': 111}\n",
      "--- Deepseek JSON response ---\n",
      "{'message': \"You're doing great! Every step forward, no matter how small, is progress. Keep going!\", 'length': 78}\n",
      "--- Gemini JSON response ---\n",
      "{'message': 'You are capable of achieving great things.', 'length': 42}\n"
     ]
    }
   ],
   "source": [
    "# define a simple schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"message\": {\"type\": \"string\"},\n",
    "        \"length\": {\"type\": \"integer\"}\n",
    "    },\n",
    "    \"required\": [\"message\", \"length\"]\n",
    "}\n",
    "\n",
    "json_prompt = \"Provide a JSON object containing an encouraging message and its character count.\"\n",
    "\n",
    "for client, name in [(openai_client, 'OpenAI'), (deepseek_client, 'Deepseek'), (gemini_client, 'Gemini')]:\n",
    "    print(f\"--- {name} JSON response ---\")\n",
    "    try:\n",
    "        result = client.generate_json(json_prompt, schema=schema)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(\"error\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62cb02",
   "metadata": {},
   "source": [
    "## 7. Handle Errors and Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing key example\n",
    "os.environ.pop('OPENAI_API_KEY', None)\n",
    "try:\n",
    "    LLMFactory.create_client(ModelProvider.OPENAI)\n",
    "except ValueError as e:\n",
    "    print(\"Expected error for missing key:\", e)\n",
    "\n",
    "# JSON parsing failure simulation: send bad prompt\n",
    "os.environ['OPENAI_API_KEY'] = 'dummy'\n",
    "class DummyClient(OpenAIClient):\n",
    "    def generate_text(self, prompt, max_tokens=None, system_prompt=None):\n",
    "        return \"not a json\"\n",
    "\n",
    "bad_client = DummyClient(api_key='abc', model_name='gpt-4o')\n",
    "try:\n",
    "    bad_client.generate_json('anything', schema={})\n",
    "except ValueError as e:\n",
    "    print(\"Caught JSON parse error:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
