{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edce7349",
   "metadata": {},
   "source": [
    "# Extract Key Points from Textbook Chunks\n",
    "\n",
    "This notebook processes the chunked textbook data and extracts key points from each section using rotating LLM providers (OpenAI, Deepseek, Gemini)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d9a71",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add repo to path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"\")))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import LLM clients\n",
    "from src.core.llm_client import ModelProvider, LLMFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5886e71",
   "metadata": {},
   "source": [
    "## 2. Load Chunks Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ca148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the chunks file\n",
    "chunks_file = Path(\"../data/hizan/output/pyhton_short-1772218124093/hybrid_auto/chunks.json\")\n",
    "\n",
    "# Load chunks\n",
    "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "print(f\"First chunk keys: {chunks[0].keys() if chunks else 'N/A'}\")\n",
    "\n",
    "# Display sample chunk\n",
    "if chunks and len(chunks) > 10:\n",
    "    sample = chunks[10]\n",
    "    print(f\"\\nSample chunk:\")\n",
    "    print(f\"Content length: {len(sample.get('content', ''))}\")\n",
    "    print(f\"Header 1: {sample.get('metadata', {}).get('header_1', 'N/A')}\")\n",
    "    print(f\"Content preview: {sample.get('content', '')[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a7449",
   "metadata": {},
   "source": [
    "## 3. Create Key Points Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_extraction_prompt(content: str) -> str:\n",
    "    \"\"\"Create a prompt for extracting key points\"\"\"\n",
    "    return f\"\"\"Extract 3-5 concise key points from the following textbook section. \n",
    "Format the output as a JSON object with a \"key_points\" array.\n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\n",
    "Return ONLY valid JSON in this format:\n",
    "{{\"key_points\": [\"point 1\", \"point 2\", \"point 3\"]}}\n",
    "\"\"\"\n",
    "\n",
    "# Test the prompt\n",
    "sample_content = chunks[15][\"content\"] if len(chunks) > 15 else \"Sample text\"\n",
    "print(\"Sample prompt:\")\n",
    "print(create_extraction_prompt(sample_content[:200])[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e561dd",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM Clients with Rotating Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clients\n",
    "try:\n",
    "    openai_client = LLMFactory.create_client(ModelProvider.OPENAI, temperature=0.3)\n",
    "    deepseek_client = LLMFactory.create_client(ModelProvider.DEEPSEEK, temperature=0.3)\n",
    "    gemini_client = LLMFactory.create_client(ModelProvider.GOOGLE, temperature=0.3)\n",
    "    \n",
    "    clients = [openai_client, deepseek_client, gemini_client]\n",
    "    client_names = [\"OpenAI\", \"Deepseek\", \"Gemini\"]\n",
    "    \n",
    "    print(\"✓ All clients initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing clients: {e}\")\n",
    "    clients = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccac487",
   "metadata": {},
   "source": [
    "## 5. Extract Key Points with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e62e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "failed_chunks = []\n",
    "\n",
    "# Process only chunks with meaningful content\n",
    "processing_chunks = [c for c in chunks if c.get('content') and len(c.get('content', '')) > 100]\n",
    "\n",
    "print(f\"Processing {len(processing_chunks)} chunks with meaningful content...\\n\")\n",
    "\n",
    "for idx, chunk in enumerate(processing_chunks):\n",
    "    # Rotate through clients\n",
    "    client = clients[idx % len(clients)]\n",
    "    client_name = client_names[idx % len(client_names)]\n",
    "    \n",
    "    content = chunk.get('content', '')\n",
    "    metadata = chunk.get('metadata', {})\n",
    "    header_1 = metadata.get('header_1', 'Unknown')\n",
    "    \n",
    "    # Skip if content is too short\n",
    "    if len(content) < 100:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Truncate long content to avoid token limits (use first 2000 chars)\n",
    "        truncated_content = content[:2000]\n",
    "        prompt = create_extraction_prompt(truncated_content)\n",
    "        \n",
    "        # Generate key points\n",
    "        response_text = client.generate_text(\n",
    "            prompt, \n",
    "            max_tokens=200,\n",
    "            system_prompt=\"You are a helpful assistant that extracts key points from educational content.\"\n",
    "        )\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        try:\n",
    "            response_json = json.loads(response_text)\n",
    "            key_points = response_json.get('key_points', [])\n",
    "        except json.JSONDecodeError:\n",
    "            # If JSON parsing fails, try to extract from markdown code blocks\n",
    "            if '```json' in response_text:\n",
    "                json_str = response_text.split('```json')[1].split('```')[0]\n",
    "                response_json = json.loads(json_str)\n",
    "                key_points = response_json.get('key_points', [])\n",
    "            elif '```' in response_text:\n",
    "                json_str = response_text.split('```')[1].split('```')[0]\n",
    "                response_json = json.loads(json_str)\n",
    "                key_points = response_json.get('key_points', [])\n",
    "            else:\n",
    "                raise ValueError(f\"Could not parse response: {response_text}\")\n",
    "        \n",
    "        results.append({\n",
    "            'header': header_1,\n",
    "            'source': metadata.get('source', ''),\n",
    "            'provider': client_name,\n",
    "            'key_points': key_points,\n",
    "            'chunk_index': idx\n",
    "        })\n",
    "        \n",
    "        if (idx + 1) % 5 == 0:\n",
    "            print(f\"✓ Processed {idx + 1}/{len(processing_chunks)} - {header_1[:40]}... ({client_name})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        failed_chunks.append({\n",
    "            'index': idx,\n",
    "            'header': header_1,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        print(f\"✗ Failed chunk {idx}: {header_1[:40]}... - {str(e)[:60]}\")\n",
    "\n",
    "print(f\"\\n✓ Extraction complete!\")\n",
    "print(f\"  Successfully processed: {len(results)}\")\n",
    "print(f\"  Failed: {len(failed_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21719be1",
   "metadata": {},
   "source": [
    "## 6. Save Results to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = Path(\"../data/hizan/output/keypoints_extracted.json\")\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'metadata': {\n",
    "            'total_chunks_processed': len(processing_chunks),\n",
    "            'total_successful': len(results),\n",
    "            'total_failed': len(failed_chunks),\n",
    "            'providers_used': client_names\n",
    "        },\n",
    "        'keypoints': results,\n",
    "        'failed': failed_chunks\n",
    "    }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_file}\")\n",
    "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f95883",
   "metadata": {},
   "source": [
    "## 7. Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "print(\"Sample extracted key points:\\n\")\n",
    "for i, result in enumerate(results[:5]):\n",
    "    print(f\"{i+1}. {result['header']} ({result['provider']})\")\n",
    "    for point in result['key_points']:\n",
    "        print(f\"   • {point}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
